---
layout: post
title:  "Why are policy gradients on-policy?"
date:   2025-07-26 21:19:42 -0700
---

In RL we often refer to algorithms as being on-policy or off-policy. What we're getting at with these terms is, can we train our policy using data generated by some _other_ policy? This comes up a lot because gathering on-policy data at every training step can be quite expensive, whereas we generally have a wealth of free off-policy data from earlier rollouts in our training run, not to mention any expert demonstrations we may have access to. For this reason, algorithms that can learn from off-policy data (DQN, DDPG, SAC, etc) are very convenient.

Of course in practice, the distinction between off-policy vs on-policy is less of a black-and-white thing and more of a spectrum. All machine learning algorithms are affected to a greater or lesser extent by the distribution of the training data, and things tend to work best when the training and evaluation distributions are the same. In this way, all RL algorithms are on-policy to some degree. That said, it's true that algorithms like DQN can generally learn just fine from data collected under a random policy a la epsilon-greedy, while policy gradients cannot, and that DQN generally benefits from huge replay buffers, while for policy gradients any stale transition data often hurts more than helps. So what is it about policy gradients that prevents us from making better use of all this off-policy data we have lying around?


# MNIST Bandit

We'll start our exploration with the normal "hello world" example: Classifying MNIST digits. To keep things simple, we'll start with a binary classification problem of whether each digit is >= 5 or < 5. In supervised learning, we'd use binary cross entropy as our loss function:

```python
observation, target = random.choice(dataset)
pred = model(observation)
loss = F.binary_cross_entropy_with_logits(pred, target >= 5)
```

For RL, we can reformulate the task as a contextual bandit problem with two possible actions, and a reward of either 1 or -1 depending on whether we classified correctly or not. The question we want to explore is, what happens if we try to train using data collected under a random policy?

Let's look at Q-learning first as an example. This case is so pathalogically simple that calling it Q-learning is a bit of a stretch; it's just straight-up regression, with no TD-updates to speak of. Our model predicts the reward for each of the two actions, we mask out the action not taken, and train with MSE against the observed reward. This whole setup is essentially just supervised learning with one of the two outputs randomly masked out, so of course it works perfectly well.

```python
observation, target = random.choice(dataset)
action = random.randint(0, 1)
reward = 1 if (action > 0) == (target >= 5) else -1
pred = dqn(observation)
loss = F.mse_loss(pred[action], reward)
```

With policy gradients, the situation is different. When we try to train with the vanilla policy gradient loss, our model NaNs out almost immediately.

```python
logit = policy(observation)
prob = F.sigmoid(logit)
prob = prob if action > 0 else (1 - prob)
loss = -reward * torch.log(prob)
```

To me, looking at this code for the first time, it wasn't immediately obvious why it shouldn't work. Of course there's the normal theoretical justification, that we're computing the gradient of expected returns with respect to the policy that actually _generated_ those returns, and so we shouldn't expect those gradients to be accurate for some other policy. But _how_, exactly, are those gradients inaccurate? It's certainly directionally correct; when the true label is 1, gradient descent will push our logits higher, and vice versa. Naively it doesn't really seem like it would matter what action distribution we collect our data under. But sure enough, if we modify our code to use on-policy actions instead of random actions, e.g. `action = random.uniform() < F.sigmoid(policy(observation))`, our model now trains without a hitch. So what gives?


# Gradients

To get a better understanding of what's going on here, we'd like to work out the exact expression for our gradients. For comparison, let's start by deriving the gradient for the binary cross entropy loss. $$x$$ is the logit and $$y$$ is a label of either 0 or 1.

{::nomarkdown}
\[
\begin{align*}
\sigma(x) &= \frac{1}{1 + e^{-x}} \\
\\
\ell_{BCE}
    &= -y \log(\sigma(x)) - (1 - y) \log(1 - \sigma(x)) \\
    &= -y \log\left(\frac{1}{1 + e^{-x}}\right) - (1 - y) \log\left(1 - \frac{1}{1 + e^{-x}}\right) \\
    &= y \log(1 + e^{-x}) - (1 - y) \log\left(\frac{e^{-x}}{1 + e^{-x}}\right) \\
    &= y \log(1 + e^{-x}) - (1 - y) \left(-x - \log(1 + e^{-x})\right) \\
    &= y \log(1 + e^{-x}) + (1 - y)x + (1 - y)\log(1 + e^{-x}) \\
    &= x(1 - y) + \log(1 + e^{-x}) \\
\\
\frac{d\ell_{BCE}}{dx}
    &= \frac{d}{dx} \left[ x(1 - y) + \log(1 + e^{-x}) \right] \\
    &= (1 - y) + \frac{1}{1 + e^{-x}} \cdot \frac{d}{dx}(1 + e^{-x}) \\
    &= (1 - y) - \frac{e^{-x}}{1 + e^{-x}} \\
    &= (1 - y) - (1 - \sigma(x)) \\
    &= \sigma(x) - y
\end{align*}
\]
{:/}

The gradient here works out to a pretty simple expression, and you can see exactly what it's doing. $$\sigma(x)$$ is between 0 and 1, so when $$y$$ is 1 the gradient will be negative, and gradient descent will push x higher; likewise when $$y$$ is 0, the gradient will be positive.

Now let's see what's going on with the policy gradient loss. $$r$$ is the return, in our case either 1 or -1. We get different expressions depending on whether the sampled action $$a$$ is 0 or 1, so we derive both cases.

{::nomarkdown}
\[
\begin{align*}
\ell_{PG \vert a=1} &= -r \cdot \log(\sigma(x)) \\
\ell_{PG \vert a=0} &= -r \cdot \log(1 - \sigma(x)) \\
\\
\frac{d\sigma}{dx}
    &= \frac{d}{dx} \left[ \frac{1}{1 + e^{-x}} \right] \\
    &= -\frac{1}{(1 + e^{-x})^2} \cdot \frac{d}{dx}(1 + e^{-x}) \\
    &= -\frac{1}{(1 + e^{-x})^2} \cdot (-e^{-x}) \\
    &= -\frac{1}{1 + e^{-x}} \cdot \frac{-e^{-x}}{1 + e^{-x}} \\
    &= \sigma(x) (1 - \sigma(x)) \\
\\
\frac{d\ell_{PG \vert a=1}}{dx}
    &= \frac{d\ell_{PG \vert a=1}}{d \sigma} \cdot \frac{d \sigma}{dx} \\
    &= -\frac{r}{\sigma(x)} \cdot \sigma(x)(1 - \sigma(x)) \\
    &= r \sigma(x) - r \\
\\
\frac{d\ell_{PG \vert a=0}}{dx}
    &= \frac{d\ell_{PG \vert a=0}}{d \sigma} \cdot \frac{d \sigma}{dx} \\
    &= -\frac{r}{1 - \sigma(x)} \cdot -\sigma(x)(1 - \sigma(x)) \\
    &= r \sigma(x)
\end{align*}
\]
{:/}

You'll immediately notice the similarity to the BCE gradient. I like [Karpathy's](https://karpathy.github.io/2016/05/31/rl/) interpretation, that what we're essentially doing here is using the sampled action as a "fake label" $$y = a$$ and computing the gradient for that, then scaling that gradient by the return. And indeed, when $$r = 1$$, our gradients exactly match their BCE equivalents. If we scale the return up or down, we scale the gradient accordingly, which seems quite reasonable on the face of it.

Where we run into problems, though, is with negative returns. Imagine we have a data point where $$y = 0$$ but the sampled action was $$a = 1$$, yielding a return of -1. With the BCE loss, our gradient is $$\sigma(x)$$ when $$y = 0$$. But the whole point of policy gradients is that we don't have access to the true label; all we can do is take the gradient for the sampled action $$a = 1$$ and scale it by the return $$r = -1$$. And here we come to the crux of the whole thing: $$\frac{d\ell_{PG \vert \text{a=1, r=-1}}}{dx} = -\sigma(x) + 1$$ and $$\frac{d\ell_{PG \vert \text{a=0, r=1}}}{dx} = \sigma(x)$$. Depending on which action we sampled, we get two entirely different gradients for the same ground-truth label! The million dollar question is, does this actually matter?


# Negative Advantage

To get a better idea of what these gradients look like, let's break out plotly and draw some plots.

{::nomarkdown}
<div style="display: flex; justify-content: center;">
  <div id="bce-grad" style="width: 500px;"></div>
</div>
<script>
(function(){
  const sigmoid = x => 1 / (1 + Math.exp(-x));
  const x = _.range(0, 100).map(i => -10 + (20 * i) / 99);
  const y = _.range(0, 100).map(i => i / 99);
  surfaceplot({x, y, z: (x, y) => sigmoid(x) - y, title: 'BCE: dℓ/dx = σ(x) - y', colorscale: plasma}, 'bce-grad');
})();
</script>
{:/}

Note what happens to the gradient at the four corners of the plot:

- True positives and true negatives are at (10, 1) and (-10, 0). These both have a gradient of 0, so x will be left unchanged.
- False positives are at (10, 0) with a gradient of 1, which will push x down.
- False negatives are at (-10, 1) with a gradient of -1, which will push x up.

So far so good! Now let's look at the policy gradient:

{::nomarkdown}
<div style="display: flex; justify-content: center; flex-wrap: wrap;">
  <div id="pg-grad-a" style="width: 500px;"></div>
  <div id="pg-grad-b" style="width: 500px;"></div>
</div>
<script>
(function(){
  const sigmoid = x => 1 / (1 + Math.exp(-x));
  const x = _.range(0, 100).map(i => -10 + (20 * i) / 99);
  const y = _.range(0, 100).map(i => -1 + (2 * i) / 99);
  surfaceplot({x, y, z: (x, y) => y * sigmoid(x) - y, title: 'PG(a = 1): dℓ/dx = rσ(x) - r', colorscale: viridis}, 'pg-grad-a');
})();
</script>
{:/}

I've omitted the plot for $$a = 0$$, since it's just a mirror image of the plot above. At $$r = 1$$, the curve matches the BCE gradient exactly, as we expected. But when $$r = -1$$, the sigmoid curve is completely backwards, which results in some strange training dynamics. This half of the plot where A < 0 corresponds to the false positives and true negatives. Let's consider these cases separately:

- At (10, -1) we have the false positives. In this case our policy incorrectly wants to take the sampled action 1 (y = 1), which yielded a negative reward. But instead of a gradient of 1, this part of the graph approaches 0 as x tends to infinity. It's still directionally correct, but it seems very counterintuitive that our gradient gets _smaller_ the more confidently wrong our policy is.
- At (-10, -1) we have the true negatives. In this case our policy correctly wants to take action 0 (y = 0), but the action we sampled was 1 (y = 1), also leading to a negative reward. Here the situation is even worse; instead of a negative feedback loop like in BCE, which damps the gradient to 0 as the model logit grows, the gradient instead approaches 1. This causes a _positive_ feedback loop.

This finally brings us back around to the initial question of why policy gradients struggle to learn from off-policy data. The saving grace of on-policy data is that it exactly counteracts the problematic gradients; as the policy logits move (correctly) towards -∞, we'll sample action 1 less and less often, which effectively downweights that data and the corresponding gradients. Likewise, as the policy logits move (incorrectly) towards ∞, we'll sample action 1 more often and those gradients will be upweighted. As long as the gradients are _directionally_ correct (which they are), using on-policy data will correct for the fact that the gradient _magnitudes_ are completely out of whack.


# Categorical Cross Entropy

Is this problem specific to logistic outputs, or does it common across different loss functions? To answer this, let's take a look at the multiclass setting. Same deal as before, we'll derive the gradients for categorical cross entropy first.

{::nomarkdown}
\[
\begin{align*}
\sigma(x)_i &= \frac{e^{x_i}}{\sum_j e^{x_j}} \\
\\
\ell_{CE}
    &= -\sum_i y_i \log(\sigma(x)_i) \\
    &=  -\sum_i y_i \log\left(\frac{e^{x_i}}{\sum_j e^{x_j}}\right) \\
    &=  -\sum_i y_i \left(x_i - \log\sum_j e^{x_j}\right) \\
    &=  -\sum_i y_i x_i + \sum_i y_i \log\sum_j e^{x_j} \\
    &=  -\sum_i y_i x_i + \log\sum_j e^{x_j} \\
\\
\frac{\partial \ell_{CE}}{\partial x_i}
    &= \frac{\partial}{\partial x_i} \left[-y_i x_i + \log\sum_j e^{x_j}\right] \\
    &= -y_i + \frac{1}{\sum_j e^{x_j}} \cdot \frac{\partial}{\partial x_i} \left[\sum_j e^{x_j}\right] \\
    &= -y_i + \frac{1}{\sum_j e^{x_j}} \cdot e^{x_i} \\
    &= \sigma(x)_i - y_i\\
\end{align*}
\]
{:/}

You'll notice the family resemblance to the BCE loss. Now for the policy gradient; once again, there are different gradient expressions for the output of the sampled action $$a$$ and for all the other outputs, and we'll derive both cases.

{::nomarkdown}
\[
\begin{align*}
\ell_{PG} &= -r \cdot \log(\sigma(x)_a) \\
\\
\frac{\partial \sigma_i}{\partial x_i}
    &= \frac{\partial}{\partial x_i} \left[ \frac{e^{x_i}}{\sum_k e^{x_k}} \right] \\
    &= \frac{e^{x_i} \cdot \sum_k e^{x_k} - e^{x_i} \cdot e^{x_i}}{\left( \sum_k e^{x_k} \right)^2} \\
    &= \sigma(x)_i - \sigma(x)_i^2 \\
    &= \sigma(x)_i \cdot (1 - \sigma(x)_i) \\
\\
\frac{\partial \sigma_i}{\partial x_{j \neq i}}
    &= \frac{\partial}{\partial x_j} \left[ \frac{e^{x_i}}{\sum_k e^{x_k}} \right] \\
    &= -\frac{e^{x_i} e^{x_j}}{\left( \sum_k e^{x_k} \right)^2} \\
    &= -\sigma(x)_i * \sigma(x)_j \\
\\
\frac{\partial \ell_{PG}}{\partial x_a}
    &= \frac{\partial \ell_{PG}}{\partial \sigma(x)_a} \cdot \frac{\partial \sigma(x)_a}{\partial x_a} \\
    &= -\frac{r}{\sigma(x)_a} \cdot \sigma(x)_a (1 - \sigma(x)_a) \\
    &= r \sigma(x)_a - r \\
\\
\frac{\partial \ell_{PG}}{\partial x_{b \neq a}}
    &= \frac{\partial \ell_{PG}}{\partial \sigma(x)_a} \cdot \frac{\partial \sigma(x)_a}{\partial x_b} \\
    &= -\frac{r}{\sigma(x)_a} \cdot -\sigma(x)_a \sigma(x)_b \\
    &= r \sigma(x)_b
\end{align*}
\]
{:/}

Well, that looks familiar! And we end up in the same situation as before: If actions $$a$$ and $$b$$ have returns of $$-1$$ and $$1$$, respectively, the gradient $$\frac{\partial \ell_{PG}}{\partial x_a}$$ will be either $$-\sigma(x)_a + 1$$ or $$\sigma(x)_a$$ depending on which action we happened to sample. $$\sigma(x)_a$$ has the desired negative feedback, but $$-\sigma(x)_a + 1$$ has positive feedback, so we get instability.


# Workarounds

A number of ideas have been proposed over the years to mitigate this problem, the most popular of which is proximal policy optimization (PPO). In its most common form, PPO masks out the gradient of any samples where the log probs of the current policy are too far away from the policy that originally collected the data, and where the gradients would us even further away. There are plenty of great articles covering the details of PPO, so I won't go into much detail here, but you can see how this breaks the feedback loop by preventing our policy from deviating too far from the original one.

PPO is a bit of a crude approach, but in practice it works surprisingly well. Plugging it into our original MNIST bandit gets us within spitting distance of the supervised baseline, using entirely off-policy data. I expect we'll eventually see further algorithm improvements in this area, but PPO is over 8 years old at this point and still going strong, so only time will tell.